# -*- coding: utf-8 -*-
"""ProjectML.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_GEidIAUyoxvHL6qjOE0GH15rZXR0eMp

# **1.Importing needed packages and explain their uses in your code.**
"""

# 1. Importing needed packages and explain their uses in your code.
import numpy as np  # for numerical operations
#Pandas offers data structure and operations for powerful, flexible, and easy-to-use data analysis and manipulation
import pandas as pd  # for data manipulation -
import matplotlib.pyplot as plt  # for data visualization
from sklearn.model_selection import train_test_split  # for data splitting
from sklearn.preprocessing import MinMaxScaler, StandardScaler  # for feature scaling
from sklearn.svm import SVC  # for Support Vector Classifier
from sklearn.metrics import accuracy_score,confusion_matrix, classification_report  # for evaluation
import seaborn as sns # Seaborn is a library for making statistical graphics in Python.
# Scikit-learn's DecisionTreeClassifier to build the decision tree model
from sklearn.tree import DecisionTreeClassifier
# Import the plot_tree function from sklearn.tree for visualizing decision tree models
from sklearn.tree import plot_tree
# Estimate Knn model and report the outcome :
from sklearn.neighbors import KNeighborsClassifier

"""# **2. Importing the selected dataset and Explore & visualizing the dataset contents.**  """

# Load the dataset
# Define the column names based on adult.names file
column_names = [
    "age", "workclass", "fnlwgt", "education", "education-num", "marital-status",
    "occupation", "relationship", "race", "sex", "capital-gain", "capital-loss",
    "hours-per-week", "native-country", "income"]

# Load the testing data (adult.test), skipping the first row containing metadata
train_data = pd.read_csv(
    "/content/drive/MyDrive/ProjectML/adult.data",
    # Path to the file to be read (adult.test)
    header=None,                    # No header row in the file, as column names are not provided in the file
    names=column_names,             # Assign the custom column names to the DataFrame from the `column_names` list
    na_values=" ?",                 # Treat any occurrences of " ?" in the dataset as missing values (NaN)
    skipinitialspace=True,          # Ignore any extra spaces after delimiters (commas) when parsing the file
    skiprows=1                      # Skip the first row in the file (metadata like "|1x3 Cross validator")
)
# Load the testing data (adult.test), skipping the first row containing metadata
test_data = pd.read_csv(
    "/content/drive/MyDrive/ProjectML/adult.test",
    # Path to the file to be read (adult.test)
    header=None,                    # No header row in the file, as column names are not provided in the file
    names=column_names,             # Assign the custom column names to the DataFrame from the `column_names` list
    na_values=" ?",                 # Treat any occurrences of " ?" in the dataset as missing values (NaN)
    skipinitialspace=True,          # Ignore any extra spaces after delimiters (commas) when parsing the file
    skiprows=1                      # Skip the first row in the file (metadata like "|1x3 Cross validator")
)

# Check the results
print("Train Data count:")
print(train_data.shape[0])
print("\nTest Data count :")
print(test_data.shape[0])

# count if there are null value in cloulm
train_data.isna().sum()

train_data

# replace ? value to null valu
train_data.replace('?', None, inplace=True)
test_data.replace('?', None, inplace=True)

# count if there are null value in cloulm
train_data.isna().sum()

"""# **Explore Training Data**"""

# Display a summary of the DataFrame, including the data types of each column,
# the number of non-null values, and memory usage
train_data.info() # display dataset information

train_data.isna().sum() # count if there are null value in cloulm

# Count the occurrences of each class (<=50K, >50K) in the 'income' column
train_data.income.value_counts()

# Visualize stroke distribution
sns.countplot(x='income', data=train_data)
plt.show()

# Visualize age distribution
plt.figure(figsize=(8, 6))
sns.histplot(train_data['age'], bins=20, kde=True, color='skyblue')
plt.title('Distribution of Age')
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.show()

sns.countplot(x='income', hue='sex', data=train_data)
plt.show()

"""# Explore Testing Data ========================================="""

test_data.isna().sum() # count if there are null value in cloulm

# Visualize income distribution
sns.countplot(x='income', data=test_data)
plt.show()

test_data['income'] = test_data['income'].replace('<=50K.', '<=50K')
test_data['income'] = test_data['income'].replace('>50K.', '>50K')

# Visualize income distribution
sns.countplot(x='income', data=test_data)
plt.show()

sns.pairplot(test_data)

"""Plot pairwise relationships in a dataset.

By default, this function will create a grid of Axes such that each numeric variable in data will by shared across the y-axes across a single row and the x-axes across a single column. The diagonal plots are treated differently: a univariate distribution plot is drawn to show the marginal distribution of the data in each column

# **Data Preprocessing**
"""

# Assuming you have your training and test data in train_data and test_data
combined_data = pd.concat([train_data, test_data], ignore_index=True)
combined_data.head(4)

combined_data.shape[0]

combined_data.isna().sum() # count if there are null value in cloulm

"""# Fill Missing Values Based on the Most Frequent Category

**Handling Missing Values Before One-Hot Encoding (Recommended Approach)**
Impute Using the Mode (Most Frequent Value):
"""

# Fill missing values in 'workclass' with the mode (most frequent value)
mode_value_workclass = combined_data['workclass'].mode()[0]
combined_data['workclass'] = combined_data['workclass'].fillna(mode_value_workclass)

# Fill missing values in 'occupation' with the mode (most frequent value)
mode_value_occupation = combined_data['occupation'].mode()[0]
combined_data['occupation'] = combined_data['occupation'].fillna(mode_value_occupation)

# Fill missing values in 'native-country' with the mode (most frequent value)
mode_value_native_country = combined_data['native-country'].mode()[0]
combined_data['native-country'] = combined_data['native-country'].fillna(mode_value_native_country)

combined_data.isna().sum() # count if there are null value in cloulm

combined_data

"""# **Encoding Categorical Variables**

"""

#One Hot Encoding Categorical Variables
#get all categorical columns
cat_columns = combined_data.select_dtypes(['object']).columns

#convert all categorical columns to numeric
combined_data[cat_columns] = combined_data[cat_columns].apply(lambda x: pd.factorize(x)[0])

#print head of data after convert
combined_data

# Visualize income distribution
sns.countplot(x='income', data=combined_data)
plt.show()

"""# Optional Steps:

 Applying Normalization to the Adult Income Dataset


"""

# Separate features and target
X_combined_data = combined_data.drop('income', axis=1)
y_combined_data = combined_data['income']

# Apply Min-Max Normalization to features
scaler = MinMaxScaler()
X_combined_data_normalized = scaler.fit_transform(X_combined_data)


# Convert back to DataFrame for better readability
combined_data= pd.DataFrame(X_combined_data_normalized, columns=X_combined_data.columns)

combined_data =combined_data.join(y_combined_data)

combined_data

"""# **3. Splitting the dataset into train, test dataset & Feature Scaling**"""

#2. Separate the Combined Data Back Into Train and Test
# Get the original number of rows for the training data
train_size = len(train_data)

# Separate the combined data back into train and test sets
train_data_separated = combined_data.iloc[:train_size]
test_data_separated = combined_data.iloc[train_size:]

# Check the results
print("Train Data Separated:")
print(train_data_separated.shape[0])
print("\nTest Data Separated:")
print(test_data_separated.shape[0])

# Separate features and target
X_train = train_data_separated.drop('income', axis=1)
y_train = train_data_separated['income']
X_test = test_data_separated.drop('income', axis=1)
y_test = test_data_separated['income']

y_train.head(2)

"""# **4. Setting up the selected SVM ML model.**"""

# Initialize the SVM classifier with default parameters
SVM_classifier = SVC()

# Alternatively, you can specify the kernel explicitly (default is 'rbf')
# classifier = SVC(kernel='rbf')
SVM_classifier = SVC(kernel='rbf', random_state=0)  # initialize SVM classifier

# You can also specify other parameters such as C, gamma, etc., but for now, let's use the defaults
# Print the default parameters of the SVM classifier
print("Default parameters of SVM classifier:")
print(SVM_classifier.get_params())

"""**Training the SVM ML model with your dataset.**"""

# Assuming X_train and y_train are your training features and labels, respectively
# Also 'classifier' is the initialized SVM classifier

# Train the SVM classifier on the training data
SVM_classifier.fit(X_train, y_train)

# Once training is complete, the model is ready to make predictions

"""**Testing and evaluating the trained SVM ML model(s) against the test dataset and report the result visually.**"""

# Make predictions on the test data
y_pred_svm = SVM_classifier.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred_svm)
print("Accuracy:", accuracy)

# Create a confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred_svm)

# Plot confusion matrix
plt.figure(figsize=(5,3))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues")
plt.title("Confusion Matrix")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.show()

# Evaluating the model
print(confusion_matrix(y_test, y_pred_svm))  # confusion matrix
print("**********************************")
print(classification_report(y_test, y_pred_svm))  # classification report

"""Based on the provided classification report:

*    **Precision**: Precision measures the proportion of true positive predictions out of all positive predictions. For class 0, the precision is 0.87, indicating that 87% of the instances predicted as class 0 are actually class 0. For class 1, the precision is 0.95, meaning that 95% of the instances predicted as class 1 are actually class 1. Higher precision values indicate better performance in correctly identifying instances of the respective class.

*    **Recall**: Recall measures the proportion of true positive predictions out of all actual positive instances. For class 0, the recall is 0.94, suggesting that 94% of the actual class 0 instances are correctly identified by the model. For class 1, the recall is 0.68, indicating that 68% of the actual class 1 instances are correctly identified by the model. Higher recall values imply better performance in capturing instances of the respective class.

*    **F1-score**: The F1-score is the harmonic mean of precision and recall, providing a balanced measure of the model's performance. It ranges from 0 to 1, with higher values indicating better overall performance. For class 0, the F1-score is 0.90, and for class 1, it is 0.79.

*    **Accuracy**: Accuracy measures the overall proportion of correctly predicted instances out of the total instances. In this case, the accuracy is 0.84, indicating that 84% of the instances are correctly classified by the model.

*    **Support**: Support represents the number of actual occurrences of each class in the test dataset. It provides context for the precision, recall, and F1-score values.

**Overall:**

*  The model achieved an accuracy of 84%, meaning that 84% of all instances were correctly classified.
*  The macro average for precision, recall, and F1-score indicates an average performance across both classes, with precision and recall slightly higher for class 0 compared to class 1.
*  The weighted average takes class imbalance into account, providing a more representative measure of overall model performance across both classes.


In summary, the model achieves high precision and recall for class 0 (indicating good performance in identifying instances of class 0), while for class 1, the recall is relatively lower, indicating that some instances of class 1 may be missed by the model. Overall, the model demonstrates good performance with an accuracy of 84%, but further analysis may be needed to understand the trade-offs between precision and recall for different classes and to identify potential areas for improvement.

# **5. Setting up the Decision tree classifier model**
Let's create a decision tree classifier model and train using Entropy as shown below:
"""

# perform training with entropy
# Decision tree with entropy
DTree_clf = DecisionTreeClassifier(criterion = "entropy", random_state = 42,max_depth = 3, min_samples_leaf = 5)
# Fit the model
DTree_clf.fit(X_train, y_train)

"""**Decision Tree Model Prediction and Performance Assessment**

"""

# Predictions on the test set
y_pred_DT = DTree_clf.predict(X_test)

# Confusion matrix and performance metrics
cm = confusion_matrix(y_test, y_pred_DT)
accuracy = accuracy_score(y_test, y_pred_DT)
report = classification_report(y_test, y_pred_DT)

print("Confusion Matrix:\n", cm)
print("Accuracy:", accuracy)
print("Classification Report:\n", report)

# Make predictions on the test data
y_pred_DT = DTree_clf.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred_DT)
print("Accuracy:", accuracy)

# Create a confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred_DT)

# Plot confusion matrix
plt.figure(figsize=(5,3))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Greens")
plt.title("Confusion Matrix")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.show()

"""**Visual Representation of the Decision Tree Model**"""

# Plot the decision tree
plt.figure(figsize=(15, 7))  # Adjust the size as needed
plot_tree(DTree_clf, feature_names=X_train.columns, class_names=['income_<=50K', 'income_>50K'], filled=True, rounded=True)

# Add title
plt.title('Decision Tree Visualization')

# Display the plot
plt.show()

"""# **6. Setting up the KNN classifier model**"""

knn = KNeighborsClassifier(n_neighbors=7)
knn.fit(X_train, y_train)

# Predictions on the test set
y_pred_knn = knn.predict(X_test)

# Confusion matrix and performance metrics
cm = confusion_matrix(y_test, y_pred_knn)
accuracy = accuracy_score(y_test, y_pred_knn)
report = classification_report(y_test, y_pred_knn)

print("Confusion Matrix:\n", cm)
print("Accuracy:", accuracy)
print("Classification Report:\n", report)

# Make predictions on the test data
y_pred_knn = knn.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred_knn)
print("Accuracy:", accuracy)

# Create a confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred_knn)

# Plot confusion matrix
plt.figure(figsize=(5, 3))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Reds")
plt.title("Confusion Matrix")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.show()